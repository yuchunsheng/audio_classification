{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50a89a5e-8bda-4d6e-8606-85d278714bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import List, Tuple, Optional\n",
    "\n",
    "# ====== 1) Your model from previous step ======\n",
    "from LogMelAffine import MelSpectrogramMatched\n",
    "\n",
    "# For this snippet, Iâ€™ll redefine a stub import path:\n",
    "# Make sure you actually import your implemented class instead\n",
    "from types import SimpleNamespace\n",
    "\n",
    "\n",
    "# ====== 2) Example dataset (replace with your own) ======\n",
    "class WaveDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Replace this with your real dataset.\n",
    "    This example returns raw (B, T) waveforms at a fixed sample rate.\n",
    "    \"\"\"\n",
    "    def __init__(self, wavs: List[torch.Tensor], sample_rate: int = 16000):\n",
    "        self.wavs = wavs\n",
    "        self.sample_rate = sample_rate\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.wavs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        wav = self.wavs[idx]\n",
    "        return wav  # shape: (T,)\n",
    "\n",
    "\n",
    "def collate_pad(batch: List[torch.Tensor]) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Pads variable-length 1D waveforms to the max length in batch and returns a mask.\n",
    "    Returns:\n",
    "        wavs: (B, T_max)\n",
    "        mask: (B, T_max) with 1.0 for valid samples else 0.0 (optional if you need masking later)\n",
    "    \"\"\"\n",
    "    lengths = [x.shape[-1] for x in batch]\n",
    "    T_max = max(lengths)\n",
    "    B = len(batch)\n",
    "    wavs = torch.zeros(B, T_max, dtype=batch[0].dtype)\n",
    "    mask = torch.zeros(B, T_max, dtype=batch[0].dtype)\n",
    "    for i, x in enumerate(batch):\n",
    "        T = x.shape[-1]\n",
    "        wavs[i, :T] = x\n",
    "        mask[i, :T] = 1.0\n",
    "    return wavs, mask\n",
    "\n",
    "\n",
    "# ====== 3) Build the torchaudio reference (target) ======\n",
    "def build_torchaudio_reference(\n",
    "    sample_rate: int,\n",
    "    n_fft: int,\n",
    "    win_length: int,\n",
    "    hop_length: int,\n",
    "    n_mels: int,\n",
    "    f_min: float,\n",
    "    f_max: Optional[float],\n",
    "    power: float,\n",
    "    norm: Optional[str],\n",
    "    mel_scale: str,\n",
    "    log_eps: float\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns a callable that maps wav -> y (log-mel target).\n",
    "    We compute torchaudio MelSpectrogram (linear), then apply torch.log with clamp (eps).\n",
    "    \"\"\"\n",
    "    mel_ref = torchaudio.transforms.MelSpectrogram(\n",
    "        sample_rate=sample_rate,\n",
    "        n_fft=n_fft,\n",
    "        win_length=win_length,\n",
    "        hop_length=hop_length,\n",
    "        f_min=f_min,\n",
    "        f_max=f_max,\n",
    "        n_mels=n_mels,\n",
    "        window_fn=torch.hann_window,   # matches periodic=True behavior\n",
    "        power=power,                   # |X|^power\n",
    "        normalized=False,\n",
    "        center=True,\n",
    "        pad_mode=\"reflect\",\n",
    "        onesided=True,\n",
    "        norm=norm,\n",
    "        mel_scale=mel_scale,\n",
    "    )\n",
    "\n",
    "    def compute_logmel(wav: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        wav: (B, T)\n",
    "        return: (B, M, frames) log-mel (natural log) with log_eps clamp\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            mel_lin = mel_ref(wav)  # (B, M, frames)\n",
    "            y = torch.log(torch.clamp_min(mel_lin, log_eps))\n",
    "        return y\n",
    "\n",
    "    return mel_ref, compute_logmel\n",
    "\n",
    "\n",
    "# ====== 4) Build your trainable model (prediction) ======\n",
    "def build_trainable_mel(\n",
    "    sample_rate: int,\n",
    "    n_fft: int,\n",
    "    win_length: int,\n",
    "    hop_length: int,\n",
    "    n_mels: int,\n",
    "    f_min: float,\n",
    "    f_max: Optional[float],\n",
    "    power: float,\n",
    "    norm: Optional[str],\n",
    "    mel_scale: str,\n",
    "    log_eps: float\n",
    ") -> nn.Module:\n",
    "    \"\"\"\n",
    "    Builds MelSpectrogramMatched with learn_affine=True.\n",
    "    All other parameters (STFT, mel) are frozen.\n",
    "    \"\"\"\n",
    "    model = MelSpectrogramMatched(\n",
    "        sample_rate=sample_rate,\n",
    "        n_fft=n_fft,\n",
    "        win_length=win_length,\n",
    "        hop_length=hop_length,\n",
    "        f_min=f_min,\n",
    "        f_max=f_max,\n",
    "        n_mels=n_mels,\n",
    "        power=power,\n",
    "        normalized=False,\n",
    "        center=True,\n",
    "        pad_mode=\"reflect\",\n",
    "        onesided=True,\n",
    "        norm=norm,\n",
    "        mel_scale=mel_scale,\n",
    "        apply_log=True,        # We want y_hat to be log-mel as well\n",
    "        log_eps=log_eps,\n",
    "        learn_affine=True,     # enable the learnable per-mel affine\n",
    "    )\n",
    "\n",
    "    # Freeze everything except affine parameters\n",
    "    for name, p in model.named_parameters():\n",
    "        if \"affine\" in name:\n",
    "            p.requires_grad = True\n",
    "        else:\n",
    "            p.requires_grad = False\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# ====== 5) Training loop ======\n",
    "def train_affine_to_match_torchaudio(\n",
    "    train_loader: DataLoader,\n",
    "    sample_rate: int = 16000,\n",
    "    n_fft: int = 1024,\n",
    "    win_length: int = 800,\n",
    "    hop_length: int = 320,\n",
    "    n_mels: int = 64,\n",
    "    f_min: float = 0.0,\n",
    "    f_max: Optional[float] = None,\n",
    "    power: float = 2.0,\n",
    "    norm: Optional[str] = None,\n",
    "    mel_scale: str = \"htk\",\n",
    "    log_eps: float = 1e-6,\n",
    "    lr: float = 1e-3,\n",
    "    weight_decay: float = 0.0,\n",
    "    max_epochs: int = 5,\n",
    "    grad_clip: Optional[float] = 5.0,\n",
    "    use_amp: bool = False,\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "):\n",
    "    # Build reference (target) and model (prediction)\n",
    "    mel_ref_module, compute_logmel = build_torchaudio_reference(\n",
    "        sample_rate, n_fft, win_length, hop_length, n_mels, f_min, f_max, power, norm, mel_scale, log_eps\n",
    "    )\n",
    "    model = build_trainable_mel(\n",
    "        sample_rate, n_fft, win_length, hop_length, n_mels, f_min, f_max, power, norm, mel_scale, log_eps\n",
    "    )\n",
    "\n",
    "    mel_ref_module.to(device)\n",
    "    model.to(device)\n",
    "\n",
    "    # Only optimize affine params\n",
    "    affine_params = [p for p in model.parameters() if p.requires_grad]\n",
    "    assert len(affine_params) > 0, \"No trainable parameters found (affine).\"\n",
    "    optimizer = torch.optim.AdamW(affine_params, lr=lr, weight_decay=weight_decay)\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=(use_amp and device.startswith(\"cuda\")))\n",
    "\n",
    "    # Loss in log-domain: MSE usually works well\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    model.train()\n",
    "    mel_ref_module.eval()  # we compute targets without grad\n",
    "\n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "        running_loss = 0.0\n",
    "        count = 0\n",
    "\n",
    "        for wavs, _mask in train_loader:\n",
    "            wavs = wavs.to(device)\n",
    "\n",
    "            # Compute target: y (log-mel from torchaudio)\n",
    "            with torch.no_grad():\n",
    "                y = compute_logmel(wavs)   # (B, M, frames)\n",
    "\n",
    "            # Compute prediction: y_hat (log-mel from our module with learnable affine)\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            if scaler.is_enabled():\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    y_hat = model(wavs)         # (B, M, frames)\n",
    "                    # Align shapes (frames may differ by 1 if padding/center differences exist; but we matched configs)\n",
    "                    # If your dataset is tricky, you can min-align:\n",
    "                    T_min = min(y.shape[-1], y_hat.shape[-1])\n",
    "                    loss = criterion(y_hat[..., :T_min], y[..., :T_min])\n",
    "            else:\n",
    "                y_hat = model(wavs)\n",
    "                T_min = min(y.shape[-1], y_hat.shape[-1])\n",
    "                loss = criterion(y_hat[..., :T_min], y[..., :T_min])\n",
    "\n",
    "            # Backward + step\n",
    "            if scaler.is_enabled():\n",
    "                scaler.scale(loss).backward()\n",
    "                if grad_clip is not None:\n",
    "                    scaler.unscale_(optimizer)\n",
    "                    torch.nn.utils.clip_grad_norm_(affine_params, grad_clip)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                loss.backward()\n",
    "                if grad_clip is not None:\n",
    "                    torch.nn.utils.clip_grad_norm_(affine_params, grad_clip)\n",
    "                optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * wavs.size(0)\n",
    "            count += wavs.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / max(count, 1)\n",
    "        print(f\"[Epoch {epoch}/{max_epochs}] train MSE: {epoch_loss:.6f}\")\n",
    "\n",
    "    print(\"Training complete.\")\n",
    "    return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ba7de28-a2c3-461b-8ce2-5890dd760996",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ycs/miniforge3/envs/audio/lib/python3.12/site-packages/torchaudio/transforms/_transforms.py:580: UserWarning: Argument 'onesided' has been deprecated and has no influence on the behavior of this module.\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_6941/120836734.py:186: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=(use_amp and device.startswith(\"cuda\")))\n",
      "/tmp/ipykernel_6941/120836734.py:209: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/3] train MSE: 0.000000\n",
      "[Epoch 2/3] train MSE: 0.000000\n",
      "[Epoch 3/3] train MSE: 0.000000\n",
      "Training complete.\n",
      "Saved learned affine weights to learned_affine_only.pt\n"
     ]
    }
   ],
   "source": [
    "# ====== 6) Example usage ======\n",
    "\n",
    "# Make a toy dataset: a few random waveforms (replace with real audio)\n",
    "sr = 16000\n",
    "ex_wavs = [torch.randn(sr * 2) for _ in range(64)]  # 2 seconds each\n",
    "\n",
    "ds = WaveDataset(ex_wavs, sample_rate=sr)\n",
    "dl = DataLoader(ds, batch_size=8, shuffle=True, collate_fn=collate_pad, num_workers=0)\n",
    "\n",
    "trained_model = train_affine_to_match_torchaudio(\n",
    "    train_loader=dl,\n",
    "    sample_rate=sr,\n",
    "    n_fft=1024,\n",
    "    win_length=800,\n",
    "    hop_length=320,\n",
    "    n_mels=64,\n",
    "    f_min=0.0,\n",
    "    f_max=sr / 2,\n",
    "    power=2.0,\n",
    "    norm=None,\n",
    "    mel_scale=\"htk\",\n",
    "    log_eps=1e-6,\n",
    "    lr=1e-3,\n",
    "    weight_decay=0.0,\n",
    "    max_epochs=3,\n",
    "    grad_clip=5.0,\n",
    "    use_amp=True,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    ")\n",
    "\n",
    "# Save learned affine parameters if you want\n",
    "state = {k: v for k, v in trained_model.state_dict().items() if \"affine\" in k}\n",
    "torch.save(state, \"learned_affine_only.pt\")\n",
    "print(\"Saved learned affine weights to learned_affine_only.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a297fa80-981e-49c3-b4b2-94ff7689185e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
