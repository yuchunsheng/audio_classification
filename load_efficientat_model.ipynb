{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6dc3d0d-58fd-49d8-86b6-b56a7299eafe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from models.mn.model import mobilenet_v3\n",
    "from helpers.utils import NAME_TO_WIDTH  # exists in EfficientAT repo\n",
    "\n",
    "pretrained_name = \"mn10_as\"       # must match the checkpoint family\n",
    "width = NAME_TO_WIDTH(pretrained_name)  # e.g., 4.0 for mn40_as\n",
    "print(width)\n",
    "\n",
    "num_classes = 527\n",
    "pretrained_name = None\n",
    "width_mult = 1.0\n",
    "reduced_tail = False\n",
    "dilated = False\n",
    "strides = (2, 2, 2, 2)\n",
    "head_type = \"mlp\"\n",
    "multihead_attention_heads = 4\n",
    "input_dim_f = 128\n",
    "input_dim_t = 300\n",
    "se_dims = 'c'\n",
    "se_agg = \"max\"\n",
    "se_r= 4\n",
    "\n",
    "input_dims = (input_dim_f, input_dim_t)\n",
    "dim_map = {'c': 1, 'f': 2, 't': 3}\n",
    "assert len(se_dims) <= 3 and all([s in dim_map.keys() for s in se_dims]) or se_dims == 'none'\n",
    "if se_dims == 'none':\n",
    "    se_dims = None\n",
    "else:\n",
    "    se_dims = [dim_map[s] for s in se_dims]\n",
    "        \n",
    "se_conf = dict(se_dims=se_dims, se_agg=se_agg, se_r=se_r)\n",
    "\n",
    "model = mobilenet_v3(pretrained_name=pretrained_name, num_classes=num_classes,\n",
    "                 width_mult=width_mult, reduced_tail=reduced_tail, dilated=dilated, strides=strides,\n",
    "                 head_type=head_type, multihead_attention_heads=multihead_attention_heads,\n",
    "                 input_dims=input_dims, se_conf=se_conf\n",
    "                 )\n",
    "\n",
    "\n",
    "# 2) Load the checkpoint you put into resources/\n",
    "ckpt_path = \"./resources/mn10_as_mels_64_mAP_461.pt\"\n",
    "sd = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "\n",
    "# 3) If head shape mismatches (e.g., your num_classes=50), load non-strict or drop head keys:\n",
    "try:\n",
    "    model.load_state_dict(sd, strict=True)\n",
    "except RuntimeError:\n",
    "    model.load_state_dict(sd, strict=False)  # classifier will be ignored if shapes differ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5998e47-b743-48f5-93a9-c57f7c88e2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model)\n",
    "# print(model.classifier)\n",
    "# print(model.classifier[5])\n",
    "# print(model.features[16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bd36471-3497-483a-86f5-5c7583b30367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MN(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2dNormActivation(\n",
      "      (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Hardswish()\n",
      "    )\n",
      "    (1): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
      "          (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
      "          (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Conv2dNormActivation(\n",
      "          (0): Conv2d(64, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=72, bias=False)\n",
      "          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Conv2dNormActivation(\n",
      "          (0): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (4): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(72, 72, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=72, bias=False)\n",
      "          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): ConcurrentSEBlock(\n",
      "          (conc_se_layers): ModuleList(\n",
      "            (0): SqueezeExcitation(\n",
      "              (fc1): Linear(in_features=72, out_features=24, bias=True)\n",
      "              (fc2): Linear(in_features=24, out_features=72, bias=True)\n",
      "              (activation): ReLU()\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(72, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (5): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
      "          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): ConcurrentSEBlock(\n",
      "          (conc_se_layers): ModuleList(\n",
      "            (0): SqueezeExcitation(\n",
      "              (fc1): Linear(in_features=120, out_features=32, bias=True)\n",
      "              (fc2): Linear(in_features=32, out_features=120, bias=True)\n",
      "              (activation): ReLU()\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (6): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
      "          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): ConcurrentSEBlock(\n",
      "          (conc_se_layers): ModuleList(\n",
      "            (0): SqueezeExcitation(\n",
      "              (fc1): Linear(in_features=120, out_features=32, bias=True)\n",
      "              (fc2): Linear(in_features=32, out_features=120, bias=True)\n",
      "              (activation): ReLU()\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (7): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
      "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): Conv2dNormActivation(\n",
      "          (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (8): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(80, 200, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(200, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(200, 200, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=200, bias=False)\n",
      "          (1): BatchNorm2d(200, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): Conv2dNormActivation(\n",
      "          (0): Conv2d(200, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (9): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)\n",
      "          (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): Conv2dNormActivation(\n",
      "          (0): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (10): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)\n",
      "          (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): Conv2dNormActivation(\n",
      "          (0): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (11): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(480, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
      "          (1): BatchNorm2d(480, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): ConcurrentSEBlock(\n",
      "          (conc_se_layers): ModuleList(\n",
      "            (0): SqueezeExcitation(\n",
      "              (fc1): Linear(in_features=480, out_features=120, bias=True)\n",
      "              (fc2): Linear(in_features=120, out_features=480, bias=True)\n",
      "              (activation): ReLU()\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(112, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (12): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=672, bias=False)\n",
      "          (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): ConcurrentSEBlock(\n",
      "          (conc_se_layers): ModuleList(\n",
      "            (0): SqueezeExcitation(\n",
      "              (fc1): Linear(in_features=672, out_features=168, bias=True)\n",
      "              (fc2): Linear(in_features=168, out_features=672, bias=True)\n",
      "              (activation): ReLU()\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(112, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (13): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
      "          (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): ConcurrentSEBlock(\n",
      "          (conc_se_layers): ModuleList(\n",
      "            (0): SqueezeExcitation(\n",
      "              (fc1): Linear(in_features=672, out_features=168, bias=True)\n",
      "              (fc2): Linear(in_features=168, out_features=672, bias=True)\n",
      "              (activation): ReLU()\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(672, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (14): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)\n",
      "          (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): ConcurrentSEBlock(\n",
      "          (conc_se_layers): ModuleList(\n",
      "            (0): SqueezeExcitation(\n",
      "              (fc1): Linear(in_features=960, out_features=240, bias=True)\n",
      "              (fc2): Linear(in_features=240, out_features=960, bias=True)\n",
      "              (activation): ReLU()\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (15): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)\n",
      "          (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): ConcurrentSEBlock(\n",
      "          (conc_se_layers): ModuleList(\n",
      "            (0): SqueezeExcitation(\n",
      "              (fc1): Linear(in_features=960, out_features=240, bias=True)\n",
      "              (fc2): Linear(in_features=240, out_features=960, bias=True)\n",
      "              (activation): ReLU()\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (16): Conv2dNormActivation(\n",
      "      (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Hardswish()\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): AdaptiveAvgPool2d(output_size=1)\n",
      "    (1): Flatten(start_dim=1, end_dim=-1)\n",
      "    (2): Linear(in_features=960, out_features=1280, bias=True)\n",
      "    (3): Hardswish()\n",
      "    (4): Dropout(p=0.2, inplace=True)\n",
      "    (5): Linear(in_features=1280, out_features=50, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model.classifier[5] = nn.Linear(1280, 50)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "352aba1c-db60-4601-b0a5-e6df98261b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Export complete: mobilenetv3_audio.onnx\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.onnx\n",
    "\n",
    "# ---- 1) Set your model to eval mode ----\n",
    "model.eval()\n",
    "\n",
    "# ---- 2) Create a dummy mel-spectrogram input ----\n",
    "# Replace these dims with your actual model’s expected input_dims\n",
    "dummy_input = torch.randn(2, 1, input_dims[0], input_dims[1])  # e.g. (1, 1, 128, 1000)\n",
    "\n",
    "# ---- 3) ONNX save path ----\n",
    "onnx_path = \"mobilenetv3_audio.onnx\"\n",
    "\n",
    "# ---- 4) Export ----\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    dummy_input,\n",
    "    onnx_path,\n",
    "    export_params=True,            # store weights inside the ONNX\n",
    "    opset_version=17,              # 11–17 works; 12 is most stable for conv models\n",
    "    do_constant_folding=True,      # optimization\n",
    "    input_names=[\"mel\"],\n",
    "    output_names=[\"logits\", \"embedding\"],\n",
    "    dynamic_axes={\n",
    "        \"mel\": {0: \"batch_size\"},  # allow dynamic batch size\n",
    "        \"logits\": {0: \"batch_size\"},\n",
    "        \"embedding\": {0: \"batch_size\"}\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Export complete: {onnx_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "858d02c6-9da3-47eb-aa0d-1a05d134fe50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 50])\n",
      "torch.Size([2, 960])\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "x = torch.randn(2, 1, 66, 300)   # (B, C, mel, frames)\n",
    "with torch.no_grad():\n",
    "    logits, feats = model(x)          # EfficientAT MN returns (logits, pooled_features)\n",
    "print(logits.shape)  # expect: [2, 527] for AudioSet pretrain head\n",
    "print(feats.shape)   # expect: [2, <embed_dim>]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "109e2a89-db2f-417a-b663-a47105bbab91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "\n",
      "============================================================\n",
      "EfficientATMN STATE DICT LOAD REPORT\n",
      "============================================================\n",
      "\n",
      "✔ Matched keys (310):\n",
      "  classifier.2.bias\n",
      "  classifier.2.weight\n",
      "  features.0.0.weight\n",
      "  features.0.1.bias\n",
      "  features.0.1.num_batches_tracked\n",
      "  features.0.1.running_mean\n",
      "  features.0.1.running_var\n",
      "  features.0.1.weight\n",
      "  features.1.block.0.0.weight\n",
      "  features.1.block.0.1.bias\n",
      "  features.1.block.0.1.num_batches_tracked\n",
      "  features.1.block.0.1.running_mean\n",
      "  features.1.block.0.1.running_var\n",
      "  features.1.block.0.1.weight\n",
      "  features.1.block.1.0.weight\n",
      "  features.1.block.1.1.bias\n",
      "  features.1.block.1.1.num_batches_tracked\n",
      "  features.1.block.1.1.running_mean\n",
      "  features.1.block.1.1.running_var\n",
      "  features.1.block.1.1.weight\n",
      "  features.10.block.0.0.weight\n",
      "  features.10.block.0.1.bias\n",
      "  features.10.block.0.1.num_batches_tracked\n",
      "  features.10.block.0.1.running_mean\n",
      "  features.10.block.0.1.running_var\n",
      "  features.10.block.0.1.weight\n",
      "  features.10.block.1.0.weight\n",
      "  features.10.block.1.1.bias\n",
      "  features.10.block.1.1.num_batches_tracked\n",
      "  features.10.block.1.1.running_mean\n",
      "  features.10.block.1.1.running_var\n",
      "  features.10.block.1.1.weight\n",
      "  features.10.block.2.0.weight\n",
      "  features.10.block.2.1.bias\n",
      "  features.10.block.2.1.num_batches_tracked\n",
      "  features.10.block.2.1.running_mean\n",
      "  features.10.block.2.1.running_var\n",
      "  features.10.block.2.1.weight\n",
      "  features.11.block.0.0.weight\n",
      "  features.11.block.0.1.bias\n",
      "  features.11.block.0.1.num_batches_tracked\n",
      "  features.11.block.0.1.running_mean\n",
      "  features.11.block.0.1.running_var\n",
      "  features.11.block.0.1.weight\n",
      "  features.11.block.1.0.weight\n",
      "  features.11.block.1.1.bias\n",
      "  features.11.block.1.1.num_batches_tracked\n",
      "  features.11.block.1.1.running_mean\n",
      "  features.11.block.1.1.running_var\n",
      "  features.11.block.1.1.weight\n",
      "  features.11.block.2.conc_se_layers.0.fc1.bias\n",
      "  features.11.block.2.conc_se_layers.0.fc1.weight\n",
      "  features.11.block.2.conc_se_layers.0.fc2.bias\n",
      "  features.11.block.2.conc_se_layers.0.fc2.weight\n",
      "  features.11.block.3.0.weight\n",
      "  features.11.block.3.1.bias\n",
      "  features.11.block.3.1.num_batches_tracked\n",
      "  features.11.block.3.1.running_mean\n",
      "  features.11.block.3.1.running_var\n",
      "  features.11.block.3.1.weight\n",
      "  features.12.block.0.0.weight\n",
      "  features.12.block.0.1.bias\n",
      "  features.12.block.0.1.num_batches_tracked\n",
      "  features.12.block.0.1.running_mean\n",
      "  features.12.block.0.1.running_var\n",
      "  features.12.block.0.1.weight\n",
      "  features.12.block.1.0.weight\n",
      "  features.12.block.1.1.bias\n",
      "  features.12.block.1.1.num_batches_tracked\n",
      "  features.12.block.1.1.running_mean\n",
      "  features.12.block.1.1.running_var\n",
      "  features.12.block.1.1.weight\n",
      "  features.12.block.2.conc_se_layers.0.fc1.bias\n",
      "  features.12.block.2.conc_se_layers.0.fc1.weight\n",
      "  features.12.block.2.conc_se_layers.0.fc2.bias\n",
      "  features.12.block.2.conc_se_layers.0.fc2.weight\n",
      "  features.12.block.3.0.weight\n",
      "  features.12.block.3.1.bias\n",
      "  features.12.block.3.1.num_batches_tracked\n",
      "  features.12.block.3.1.running_mean\n",
      "  features.12.block.3.1.running_var\n",
      "  features.12.block.3.1.weight\n",
      "  features.13.block.0.0.weight\n",
      "  features.13.block.0.1.bias\n",
      "  features.13.block.0.1.num_batches_tracked\n",
      "  features.13.block.0.1.running_mean\n",
      "  features.13.block.0.1.running_var\n",
      "  features.13.block.0.1.weight\n",
      "  features.13.block.1.0.weight\n",
      "  features.13.block.1.1.bias\n",
      "  features.13.block.1.1.num_batches_tracked\n",
      "  features.13.block.1.1.running_mean\n",
      "  features.13.block.1.1.running_var\n",
      "  features.13.block.1.1.weight\n",
      "  features.13.block.2.conc_se_layers.0.fc1.bias\n",
      "  features.13.block.2.conc_se_layers.0.fc1.weight\n",
      "  features.13.block.2.conc_se_layers.0.fc2.bias\n",
      "  features.13.block.2.conc_se_layers.0.fc2.weight\n",
      "  features.13.block.3.0.weight\n",
      "  features.13.block.3.1.bias\n",
      "  features.13.block.3.1.num_batches_tracked\n",
      "  features.13.block.3.1.running_mean\n",
      "  features.13.block.3.1.running_var\n",
      "  features.13.block.3.1.weight\n",
      "  features.14.block.0.0.weight\n",
      "  features.14.block.0.1.bias\n",
      "  features.14.block.0.1.num_batches_tracked\n",
      "  features.14.block.0.1.running_mean\n",
      "  features.14.block.0.1.running_var\n",
      "  features.14.block.0.1.weight\n",
      "  features.14.block.1.0.weight\n",
      "  features.14.block.1.1.bias\n",
      "  features.14.block.1.1.num_batches_tracked\n",
      "  features.14.block.1.1.running_mean\n",
      "  features.14.block.1.1.running_var\n",
      "  features.14.block.1.1.weight\n",
      "  features.14.block.2.conc_se_layers.0.fc1.bias\n",
      "  features.14.block.2.conc_se_layers.0.fc1.weight\n",
      "  features.14.block.2.conc_se_layers.0.fc2.bias\n",
      "  features.14.block.2.conc_se_layers.0.fc2.weight\n",
      "  features.14.block.3.0.weight\n",
      "  features.14.block.3.1.bias\n",
      "  features.14.block.3.1.num_batches_tracked\n",
      "  features.14.block.3.1.running_mean\n",
      "  features.14.block.3.1.running_var\n",
      "  features.14.block.3.1.weight\n",
      "  features.15.block.0.0.weight\n",
      "  features.15.block.0.1.bias\n",
      "  features.15.block.0.1.num_batches_tracked\n",
      "  features.15.block.0.1.running_mean\n",
      "  features.15.block.0.1.running_var\n",
      "  features.15.block.0.1.weight\n",
      "  features.15.block.1.0.weight\n",
      "  features.15.block.1.1.bias\n",
      "  features.15.block.1.1.num_batches_tracked\n",
      "  features.15.block.1.1.running_mean\n",
      "  features.15.block.1.1.running_var\n",
      "  features.15.block.1.1.weight\n",
      "  features.15.block.2.conc_se_layers.0.fc1.bias\n",
      "  features.15.block.2.conc_se_layers.0.fc1.weight\n",
      "  features.15.block.2.conc_se_layers.0.fc2.bias\n",
      "  features.15.block.2.conc_se_layers.0.fc2.weight\n",
      "  features.15.block.3.0.weight\n",
      "  features.15.block.3.1.bias\n",
      "  features.15.block.3.1.num_batches_tracked\n",
      "  features.15.block.3.1.running_mean\n",
      "  features.15.block.3.1.running_var\n",
      "  features.15.block.3.1.weight\n",
      "  features.16.0.weight\n",
      "  features.16.1.bias\n",
      "  features.16.1.num_batches_tracked\n",
      "  features.16.1.running_mean\n",
      "  features.16.1.running_var\n",
      "  features.16.1.weight\n",
      "  features.2.block.0.0.weight\n",
      "  features.2.block.0.1.bias\n",
      "  features.2.block.0.1.num_batches_tracked\n",
      "  features.2.block.0.1.running_mean\n",
      "  features.2.block.0.1.running_var\n",
      "  features.2.block.0.1.weight\n",
      "  features.2.block.1.0.weight\n",
      "  features.2.block.1.1.bias\n",
      "  features.2.block.1.1.num_batches_tracked\n",
      "  features.2.block.1.1.running_mean\n",
      "  features.2.block.1.1.running_var\n",
      "  features.2.block.1.1.weight\n",
      "  features.2.block.2.0.weight\n",
      "  features.2.block.2.1.bias\n",
      "  features.2.block.2.1.num_batches_tracked\n",
      "  features.2.block.2.1.running_mean\n",
      "  features.2.block.2.1.running_var\n",
      "  features.2.block.2.1.weight\n",
      "  features.3.block.0.0.weight\n",
      "  features.3.block.0.1.bias\n",
      "  features.3.block.0.1.num_batches_tracked\n",
      "  features.3.block.0.1.running_mean\n",
      "  features.3.block.0.1.running_var\n",
      "  features.3.block.0.1.weight\n",
      "  features.3.block.1.0.weight\n",
      "  features.3.block.1.1.bias\n",
      "  features.3.block.1.1.num_batches_tracked\n",
      "  features.3.block.1.1.running_mean\n",
      "  features.3.block.1.1.running_var\n",
      "  features.3.block.1.1.weight\n",
      "  features.3.block.2.0.weight\n",
      "  features.3.block.2.1.bias\n",
      "  features.3.block.2.1.num_batches_tracked\n",
      "  features.3.block.2.1.running_mean\n",
      "  features.3.block.2.1.running_var\n",
      "  features.3.block.2.1.weight\n",
      "  features.4.block.0.0.weight\n",
      "  features.4.block.0.1.bias\n",
      "  features.4.block.0.1.num_batches_tracked\n",
      "  features.4.block.0.1.running_mean\n",
      "  features.4.block.0.1.running_var\n",
      "  features.4.block.0.1.weight\n",
      "  features.4.block.1.0.weight\n",
      "  features.4.block.1.1.bias\n",
      "  features.4.block.1.1.num_batches_tracked\n",
      "  features.4.block.1.1.running_mean\n",
      "  features.4.block.1.1.running_var\n",
      "  features.4.block.1.1.weight\n",
      "  features.4.block.2.conc_se_layers.0.fc1.bias\n",
      "  features.4.block.2.conc_se_layers.0.fc1.weight\n",
      "  features.4.block.2.conc_se_layers.0.fc2.bias\n",
      "  features.4.block.2.conc_se_layers.0.fc2.weight\n",
      "  features.4.block.3.0.weight\n",
      "  features.4.block.3.1.bias\n",
      "  features.4.block.3.1.num_batches_tracked\n",
      "  features.4.block.3.1.running_mean\n",
      "  features.4.block.3.1.running_var\n",
      "  features.4.block.3.1.weight\n",
      "  features.5.block.0.0.weight\n",
      "  features.5.block.0.1.bias\n",
      "  features.5.block.0.1.num_batches_tracked\n",
      "  features.5.block.0.1.running_mean\n",
      "  features.5.block.0.1.running_var\n",
      "  features.5.block.0.1.weight\n",
      "  features.5.block.1.0.weight\n",
      "  features.5.block.1.1.bias\n",
      "  features.5.block.1.1.num_batches_tracked\n",
      "  features.5.block.1.1.running_mean\n",
      "  features.5.block.1.1.running_var\n",
      "  features.5.block.1.1.weight\n",
      "  features.5.block.2.conc_se_layers.0.fc1.bias\n",
      "  features.5.block.2.conc_se_layers.0.fc1.weight\n",
      "  features.5.block.2.conc_se_layers.0.fc2.bias\n",
      "  features.5.block.2.conc_se_layers.0.fc2.weight\n",
      "  features.5.block.3.0.weight\n",
      "  features.5.block.3.1.bias\n",
      "  features.5.block.3.1.num_batches_tracked\n",
      "  features.5.block.3.1.running_mean\n",
      "  features.5.block.3.1.running_var\n",
      "  features.5.block.3.1.weight\n",
      "  features.6.block.0.0.weight\n",
      "  features.6.block.0.1.bias\n",
      "  features.6.block.0.1.num_batches_tracked\n",
      "  features.6.block.0.1.running_mean\n",
      "  features.6.block.0.1.running_var\n",
      "  features.6.block.0.1.weight\n",
      "  features.6.block.1.0.weight\n",
      "  features.6.block.1.1.bias\n",
      "  features.6.block.1.1.num_batches_tracked\n",
      "  features.6.block.1.1.running_mean\n",
      "  features.6.block.1.1.running_var\n",
      "  features.6.block.1.1.weight\n",
      "  features.6.block.2.conc_se_layers.0.fc1.bias\n",
      "  features.6.block.2.conc_se_layers.0.fc1.weight\n",
      "  features.6.block.2.conc_se_layers.0.fc2.bias\n",
      "  features.6.block.2.conc_se_layers.0.fc2.weight\n",
      "  features.6.block.3.0.weight\n",
      "  features.6.block.3.1.bias\n",
      "  features.6.block.3.1.num_batches_tracked\n",
      "  features.6.block.3.1.running_mean\n",
      "  features.6.block.3.1.running_var\n",
      "  features.6.block.3.1.weight\n",
      "  features.7.block.0.0.weight\n",
      "  features.7.block.0.1.bias\n",
      "  features.7.block.0.1.num_batches_tracked\n",
      "  features.7.block.0.1.running_mean\n",
      "  features.7.block.0.1.running_var\n",
      "  features.7.block.0.1.weight\n",
      "  features.7.block.1.0.weight\n",
      "  features.7.block.1.1.bias\n",
      "  features.7.block.1.1.num_batches_tracked\n",
      "  features.7.block.1.1.running_mean\n",
      "  features.7.block.1.1.running_var\n",
      "  features.7.block.1.1.weight\n",
      "  features.7.block.2.0.weight\n",
      "  features.7.block.2.1.bias\n",
      "  features.7.block.2.1.num_batches_tracked\n",
      "  features.7.block.2.1.running_mean\n",
      "  features.7.block.2.1.running_var\n",
      "  features.7.block.2.1.weight\n",
      "  features.8.block.0.0.weight\n",
      "  features.8.block.0.1.bias\n",
      "  features.8.block.0.1.num_batches_tracked\n",
      "  features.8.block.0.1.running_mean\n",
      "  features.8.block.0.1.running_var\n",
      "  features.8.block.0.1.weight\n",
      "  features.8.block.1.0.weight\n",
      "  features.8.block.1.1.bias\n",
      "  features.8.block.1.1.num_batches_tracked\n",
      "  features.8.block.1.1.running_mean\n",
      "  features.8.block.1.1.running_var\n",
      "  features.8.block.1.1.weight\n",
      "  features.8.block.2.0.weight\n",
      "  features.8.block.2.1.bias\n",
      "  features.8.block.2.1.num_batches_tracked\n",
      "  features.8.block.2.1.running_mean\n",
      "  features.8.block.2.1.running_var\n",
      "  features.8.block.2.1.weight\n",
      "  features.9.block.0.0.weight\n",
      "  features.9.block.0.1.bias\n",
      "  features.9.block.0.1.num_batches_tracked\n",
      "  features.9.block.0.1.running_mean\n",
      "  features.9.block.0.1.running_var\n",
      "  features.9.block.0.1.weight\n",
      "  features.9.block.1.0.weight\n",
      "  features.9.block.1.1.bias\n",
      "  features.9.block.1.1.num_batches_tracked\n",
      "  features.9.block.1.1.running_mean\n",
      "  features.9.block.1.1.running_var\n",
      "  features.9.block.1.1.weight\n",
      "  features.9.block.2.0.weight\n",
      "  features.9.block.2.1.bias\n",
      "  features.9.block.2.1.num_batches_tracked\n",
      "  features.9.block.2.1.running_mean\n",
      "  features.9.block.2.1.running_var\n",
      "  features.9.block.2.1.weight\n",
      "\n",
      "✘ Missing keys (0):\n",
      "\n",
      "⚠ Unexpected keys (0):\n",
      "\n",
      "❗ Shape mismatches (2):\n",
      "  classifier.5.bias: model=(50,), checkpoint=(527,)\n",
      "  classifier.5.weight: model=(50, 1280), checkpoint=(527, 1280)\n",
      "\n",
      "Done.\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def inspect_state_dict_loading(model, state_dict, prefix=\"\"):\n",
    "    \"\"\"\n",
    "    Utility to compare model parameters with those in a state_dict and print:\n",
    "      - matched keys\n",
    "      - missing keys\n",
    "      - unexpected keys\n",
    "      - shape mismatches\n",
    "    \"\"\"\n",
    "\n",
    "    model_keys = set(model.state_dict().keys())\n",
    "    ckpt_keys = set(state_dict.keys())\n",
    "\n",
    "    matched_keys = []\n",
    "    missing_keys = sorted(list(model_keys - ckpt_keys))\n",
    "    unexpected_keys = sorted(list(ckpt_keys - model_keys))\n",
    "    shape_mismatches = []\n",
    "\n",
    "    # Check matched keys and tensor shape mismatches\n",
    "    for k in sorted(list(model_keys & ckpt_keys)):\n",
    "        model_shape = tuple(model.state_dict()[k].shape)\n",
    "        ckpt_shape = tuple(state_dict[k].shape)\n",
    "        if model_shape == ckpt_shape:\n",
    "            matched_keys.append(k)\n",
    "        else:\n",
    "            shape_mismatches.append((k, model_shape, ckpt_shape))\n",
    "\n",
    "    # Pretty printing\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"{prefix} STATE DICT LOAD REPORT\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    print(f\"\\n✔ Matched keys ({len(matched_keys)}):\")\n",
    "    for k in matched_keys:\n",
    "        print(f\"  {k}\")\n",
    "\n",
    "    print(f\"\\n✘ Missing keys ({len(missing_keys)}):\")\n",
    "    for k in missing_keys:\n",
    "        print(f\"  {k}\")\n",
    "\n",
    "    print(f\"\\n⚠ Unexpected keys ({len(unexpected_keys)}):\")\n",
    "    for k in unexpected_keys:\n",
    "        print(f\"  {k}\")\n",
    "\n",
    "    print(f\"\\n❗ Shape mismatches ({len(shape_mismatches)}):\")\n",
    "    for k, ms, cs in shape_mismatches:\n",
    "        print(f\"  {k}: model={ms}, checkpoint={cs}\")\n",
    "\n",
    "    print(\"\\nDone.\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "\n",
    "import torch\n",
    "from models.mn.model import mobilenet_v3\n",
    "from helpers.utils import NAME_TO_WIDTH  # exists in EfficientAT repo\n",
    "\n",
    "pretrained_name = \"mn10_as\"       # must match the checkpoint family\n",
    "width = NAME_TO_WIDTH(pretrained_name)  # e.g., 4.0 for mn40_as\n",
    "print(width)\n",
    "\n",
    "num_classes = 527\n",
    "pretrained_name = None\n",
    "width_mult = 1.0\n",
    "reduced_tail = False\n",
    "dilated = False\n",
    "strides = (2, 2, 2, 2)\n",
    "head_type = \"mlp\"\n",
    "multihead_attention_heads = 4\n",
    "input_dim_f = 128\n",
    "input_dim_t = 1000\n",
    "se_dims = 'c'\n",
    "se_agg = \"max\"\n",
    "se_r= 4\n",
    "\n",
    "input_dims = (input_dim_f, input_dim_t)\n",
    "dim_map = {'c': 1, 'f': 2, 't': 3}\n",
    "assert len(se_dims) <= 3 and all([s in dim_map.keys() for s in se_dims]) or se_dims == 'none'\n",
    "if se_dims == 'none':\n",
    "    se_dims = None\n",
    "else:\n",
    "    se_dims = [dim_map[s] for s in se_dims]\n",
    "        \n",
    "se_conf = dict(se_dims=se_dims, se_agg=se_agg, se_r=se_r)\n",
    "\n",
    "model = mobilenet_v3(pretrained_name=pretrained_name, num_classes=num_classes,\n",
    "                 width_mult=width_mult, reduced_tail=reduced_tail, dilated=dilated, strides=strides,\n",
    "                 head_type=head_type, multihead_attention_heads=multihead_attention_heads,\n",
    "                 input_dims=input_dims, se_conf=se_conf\n",
    "                 )\n",
    "# model.classifier[5] = nn.Linear(1280, 50)\n",
    "\n",
    "state_dict = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "\n",
    "inspect_state_dict_loading(model, state_dict, prefix=\"EfficientATMN\")\n",
    "# model.load_state_dict(state_dict, strict=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee86b91-a941-4382-b9bd-9a09f8e97da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79baacbe-77fc-46ed-9b39-69ce6eb6ba53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from models.mn.model import get_model\n",
    "from helpers.utils import NAME_TO_WIDTH  # exists in EfficientAT repo\n",
    "\n",
    "pretrained_name = \"mn10_as\"       # must match the checkpoint family\n",
    "width = NAME_TO_WIDTH(pretrained_name)  # e.g., 4.0 for mn40_as\n",
    "print(width)\n",
    "\n",
    "# 1) Build the architecture with your desired head & dims\n",
    "model = get_model(\n",
    "    num_classes=527,     # or 50 if you want to initialize with a fresh classifier\n",
    "    pretrained_name=None, # <-- don't trigger URL loader\n",
    "    width_mult=width,\n",
    "    head_type=\"mlp\",\n",
    "    input_dim_f=128,\n",
    "    input_dim_t=300               # ~3s with 10ms hop ≈ 300 frames\n",
    ")\n",
    "\n",
    "# 2) Load the checkpoint you put into resources/\n",
    "ckpt_path = \"./resources/mn10_as_mels_64_mAP_461.pt\"\n",
    "sd = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "\n",
    "# 3) If head shape mismatches (e.g., your num_classes=50), load non-strict or drop head keys:\n",
    "try:\n",
    "    model.load_state_dict(sd, strict=True)\n",
    "except RuntimeError:\n",
    "    model.load_state_dict(sd, strict=False)  # classifier will be ignored if shapes differ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b222da5-11c5-4380-bebf-a067ef1e326d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "x = torch.randn(2, 1, 180, 300)   # (B, C, mel, frames)\n",
    "with torch.no_grad():\n",
    "    logits, feats = model(x)          # EfficientAT MN returns (logits, pooled_features)\n",
    "print(logits.shape)  # expect: [2, 527] for AudioSet pretrain head\n",
    "print(feats.shape)   # expect: [2, <embed_dim>]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e229e7b4-f927-42f5-b514-f7078f1ecac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from models.mn.model import get_model\n",
    "from helpers.utils import NAME_TO_WIDTH  # exists in EfficientAT repo\n",
    "\n",
    "pretrained_name = \"mn10_as\"       # must match the checkpoint family\n",
    "width = NAME_TO_WIDTH(pretrained_name)  # e.g., 4.0 for mn40_as\n",
    "print(width)\n",
    "\n",
    "# Build with the **correct width** and your head type\n",
    "# Tip: set num_classes=50 for ESC-50; the loader will drop the old classifier automatically\n",
    "model1 = get_model(\n",
    "    num_classes=50,               # ESC-50\n",
    "    pretrained_name=pretrained_name,\n",
    "    width_mult=width,\n",
    "    head_type=\"mlp\",              # or \"fully_convolutional\" if that's your choice\n",
    "    input_dim_f=128,\n",
    "    input_dim_t=300               # ~3s with 10ms hop ≈ 300 frames\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32515c6-65ad-4432-98b4-72dbdb8b9640",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "\n",
    "onnx_path = \"mobilenetv3_audio.onnx\"\n",
    "sess = ort.InferenceSession(onnx_path, providers=[\"CPUExecutionProvider\"])\n",
    "\n",
    "# Inspect model I/O\n",
    "print(\"Inputs:\")\n",
    "for i in sess.get_inputs():\n",
    "    print(\" -\", i.name, i.shape, i.type)\n",
    "\n",
    "print(\"Outputs:\")\n",
    "for o in sess.get_outputs():\n",
    "    print(\" -\", o.name, o.shape, o.type)\n",
    "\n",
    "# Prepare dummy input (match your export: (N, C, F, T))\n",
    "# Replace these with the same input_dims used at export time\n",
    "F, T = input_dims\n",
    "mel = np.random.randn(1, 1, F, T).astype(np.float32)\n",
    "\n",
    "# Run inference\n",
    "outputs = sess.run(None, {\"mel\": mel})\n",
    "\n",
    "logits = outputs[0]          # shape: (N, num_classes)\n",
    "embedding = outputs[1]       # shape: (N, last_channel)\n",
    "print(\"logits:\", logits.shape, \"embedding:\", embedding.shape)\n",
    "\n",
    "# Optional: Softmax to get probabilities\n",
    "probs = np.exp(logits - logits.max(axis=1, keepdims=True))\n",
    "probs = probs / probs.sum(axis=1, keepdims=True)\n",
    "topk_idx = probs[0].argsort()[::-1][:5]\n",
    "print(\"Top-5 class ids:\", topk_idx)\n",
    "print(\"Top-5 probs:\", probs[0][topk_idx])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2c017cf-9a4a-4c3e-9d63-94076872790c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MN(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2dNormActivation(\n",
      "      (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Hardswish()\n",
      "    )\n",
      "    (1): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
      "          (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
      "          (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Conv2dNormActivation(\n",
      "          (0): Conv2d(64, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=72, bias=False)\n",
      "          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Conv2dNormActivation(\n",
      "          (0): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (4): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(72, 72, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=72, bias=False)\n",
      "          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): ConcurrentSEBlock(\n",
      "          (conc_se_layers): ModuleList(\n",
      "            (0): SqueezeExcitation(\n",
      "              (fc1): Linear(in_features=72, out_features=24, bias=True)\n",
      "              (fc2): Linear(in_features=24, out_features=72, bias=True)\n",
      "              (activation): ReLU()\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(72, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (5): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
      "          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): ConcurrentSEBlock(\n",
      "          (conc_se_layers): ModuleList(\n",
      "            (0): SqueezeExcitation(\n",
      "              (fc1): Linear(in_features=120, out_features=32, bias=True)\n",
      "              (fc2): Linear(in_features=32, out_features=120, bias=True)\n",
      "              (activation): ReLU()\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (6): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
      "          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): ConcurrentSEBlock(\n",
      "          (conc_se_layers): ModuleList(\n",
      "            (0): SqueezeExcitation(\n",
      "              (fc1): Linear(in_features=120, out_features=32, bias=True)\n",
      "              (fc2): Linear(in_features=32, out_features=120, bias=True)\n",
      "              (activation): ReLU()\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (7): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
      "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): Conv2dNormActivation(\n",
      "          (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (8): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(80, 200, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(200, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(200, 200, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=200, bias=False)\n",
      "          (1): BatchNorm2d(200, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): Conv2dNormActivation(\n",
      "          (0): Conv2d(200, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (9): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)\n",
      "          (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): Conv2dNormActivation(\n",
      "          (0): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (10): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)\n",
      "          (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): Conv2dNormActivation(\n",
      "          (0): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (11): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(480, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
      "          (1): BatchNorm2d(480, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): ConcurrentSEBlock(\n",
      "          (conc_se_layers): ModuleList(\n",
      "            (0): SqueezeExcitation(\n",
      "              (fc1): Linear(in_features=480, out_features=120, bias=True)\n",
      "              (fc2): Linear(in_features=120, out_features=480, bias=True)\n",
      "              (activation): ReLU()\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(112, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (12): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=672, bias=False)\n",
      "          (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): ConcurrentSEBlock(\n",
      "          (conc_se_layers): ModuleList(\n",
      "            (0): SqueezeExcitation(\n",
      "              (fc1): Linear(in_features=672, out_features=168, bias=True)\n",
      "              (fc2): Linear(in_features=168, out_features=672, bias=True)\n",
      "              (activation): ReLU()\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(112, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (13): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
      "          (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): ConcurrentSEBlock(\n",
      "          (conc_se_layers): ModuleList(\n",
      "            (0): SqueezeExcitation(\n",
      "              (fc1): Linear(in_features=672, out_features=168, bias=True)\n",
      "              (fc2): Linear(in_features=168, out_features=672, bias=True)\n",
      "              (activation): ReLU()\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(672, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (14): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)\n",
      "          (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): ConcurrentSEBlock(\n",
      "          (conc_se_layers): ModuleList(\n",
      "            (0): SqueezeExcitation(\n",
      "              (fc1): Linear(in_features=960, out_features=240, bias=True)\n",
      "              (fc2): Linear(in_features=240, out_features=960, bias=True)\n",
      "              (activation): ReLU()\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (15): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)\n",
      "          (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): ConcurrentSEBlock(\n",
      "          (conc_se_layers): ModuleList(\n",
      "            (0): SqueezeExcitation(\n",
      "              (fc1): Linear(in_features=960, out_features=240, bias=True)\n",
      "              (fc2): Linear(in_features=240, out_features=960, bias=True)\n",
      "              (activation): ReLU()\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (16): Conv2dNormActivation(\n",
      "      (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Hardswish()\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): AdaptiveAvgPool2d(output_size=1)\n",
      "    (1): Flatten(start_dim=1, end_dim=-1)\n",
      "    (2): Linear(in_features=960, out_features=1280, bias=True)\n",
      "    (3): Hardswish()\n",
      "    (4): Dropout(p=0.2, inplace=True)\n",
      "    (5): Linear(in_features=1280, out_features=50, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from build_model import build_model_from_efficientat\n",
    "m = build_model_from_efficientat()\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0110f896-401d-4060-a805-6653241f9308",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
